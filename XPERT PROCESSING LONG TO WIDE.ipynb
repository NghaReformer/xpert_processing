{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dates(date_str):\n",
    "    if not isinstance(date_str, str):\n",
    "        return pd.NaT  # Return a 'Not a Timestamp' for non-string inputs (likely NaN)\n",
    "    try:\n",
    "        # dateutil's parse function will automatically infer the date format\n",
    "        parsed_date = parser.parse(date_str)\n",
    "        # Convert to pandas Timestamp\n",
    "        timestamp = pd.Timestamp(parsed_date)\n",
    "        # If the time is 00:00:00 (i.e., it's a date only), add a time of 00:00:00\n",
    "        if timestamp.time() == pd.Timestamp('00:00:00').time():\n",
    "            timestamp = timestamp.replace(hour=0, minute=0, second=0)\n",
    "        return timestamp\n",
    "    except ValueError:\n",
    "        return pd.NaT  # Return a 'Not a Timestamp' for unparseable formats\n",
    "\n",
    "\n",
    "xpert_files = pd.read_csv(\"E:\\\\CHPR_ORG\\Bamenda Center for Health Promotion and Research\\\\INSPIRE TB Cameroon - INSPIRE TB PROJECT\\\\DO NOT MODIFY\\\\prospective_xpert_export_files.csv\")\n",
    "\n",
    "# extract the folder name\n",
    "xpert_files['lab'] =  xpert_files['folder_path'].str.split('/').str[2]\n",
    "# filter  out non csv files\n",
    "xpert_files = xpert_files[xpert_files['file'].str.contains('.csv', case = False)]\n",
    "\n",
    "# drop the folder path column\n",
    "xpert_files.drop(columns = ['folder_path'], inplace = True)\n",
    "xpert_files['start_date'] = xpert_files['start_date'].apply(fix_dates)\n",
    "xpert_files['end_date'] = xpert_files['end_date'].apply(fix_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE THE LANGUAGE TRANSLATION DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_map = {\n",
    "\t'INFORMATIONS SUR LE TEST' : 'ASSAY INFORMATION',\n",
    "\t\"Paramètres d'analyse\" : 'Analysis Settings',\n",
    "\t\"TABLEAU DE RÉSULTATS\" : 'RESULT TABLE',\n",
    "\t'Indice de cartouche' : \"Cartridge Index\",\n",
    "\t'Historique'  : 'History',\n",
    "\t\"Résultat de l\\'analyte\" : 'Analyte Result',\n",
    "\t'Détail' : 'Detail',\n",
    "\t'Pics de fusion' :  'Melt Peaks',\n",
    "\t'Erreurs' : 'Errors',\n",
    "\t'Messages' : 'Messages',\n",
    "\t'Nom du module' : 'Module Name',\n",
    "\t'N° de série du module' : 'Module S/N',\n",
    "\t'Numéro de série de la cartouche' : 'Cartridge S/N', \n",
    "\t\"N° de série de l\\'instrument\": 'Instrument S/N', \n",
    "\t'Version du logiciel' : 'S/W Version',\n",
    "\t'N° du lot' : 'Reagent Lot ID',\n",
    "    'Date d\\'expiration': 'Expiration Date',\n",
    "\t'Heure de lancement': 'Start Time',\n",
    "\t'Heure de fin' : 'End Time',\n",
    "    \"État de l\\'erreur\"  : 'Error Status',\n",
    "\t'État' : 'Status',\n",
    "\t'Utilisateur' :  'User',\n",
    "\t'Effectué' : 'Done',\n",
    "\t'Incomplet' : 'Incomplete',\n",
    "\t'Interrompu' : 'Done',\n",
    "\t\n",
    "\t\"N° Id de l'échantillon\" : 'Sample ID',\n",
    "\t'N° Id du patient' : 'Patient ID', \n",
    "\t'Test' : 'Assay',\n",
    "\t'Version du test' : 'Assay Version',\n",
    "\t'Type de test' : 'Assay Type', \n",
    "\t'Type de test' : 'Test Type',\n",
    "\t\"Type d\\'échantillon\" : 'Sample Type',\n",
    "\t'Remarques' : 'Notes',\n",
    "\t'Résultat du test' : 'Test Result', \n",
    "\t'Avertissement du test' : 'Test Disclaimer',\n",
    "\t\"Date d'exportation\" : 'Exported Date',\n",
    "\t'Clause de non-responsabilité concernant le test': 'Test Disclaimer',\n",
    "\t\n",
    "\t'MTB NON DÉTECTÉ||' : 'MTB NOT DETECTED||',\n",
    "\t'MTB DÉTECTÉ ÉLEVÉ||RIF Resistance NON DÉTECTÉ' : 'MTB DETECTED HIGH||RIF Resistance NOT DETECTED',\n",
    "\t'MTB DÉTECTÉ BAS||RIF Resistance NON DÉTECTÉ' : 'MTB DETECTED LOW||RIF Resistance NOT DETECTED',\n",
    "\t'MTB DÉTECTÉ MOYEN||RIF Resistance NON DÉTECTÉ' : 'MTB DETECTED MEDIUM||RIF Resistance NOT DETECTED',\n",
    "\t'MTB Trace DÉTECTÉ|RIF Resistance INDÉTERMINÉ' : '|MTB Trace DETECTED|RIF Resistance INDETERMINATE',\n",
    "\t'MTB DÉTECTÉ ÉLEVÉ||RIF Resistance DÉTECTÉ' : 'MTB DETECTED HIGH||RIF Resistance DETECTED',\n",
    "\t'MTB DÉTECTÉ TRÈS BAS||RIF Resistance NON DÉTECTÉ' : 'MTB DETECTED VERY LOW||RIF Resistance NOT DETECTED',\n",
    "\t'MTB DÉTECTÉ TRÈS BAS||RIF Resistance DÉTECTÉ' : 'MTB DETECTED VERY LOW||RIF Resistance DETECTED',\n",
    "\t'HIV-1 NON DÉTECTÉ|' : 'HIV-1 NOT DETECTED|',\n",
    "\t'MTB DÉTECTÉ BAS||RIF Resistance DÉTECTÉ':'MTB DETECTED LOW||RIF Resistance DETECTED',\n",
    "    \n",
    "\t'MTB DÉTECTÉ MOYEN||RIF Resistance INDÉTERMINÉ': 'MTB DETECTED MEDIUM||RIF Resistance INDETERMINATE',\n",
    "\t'MTB DÉTECTÉ TRÈS BAS||RIF Resistance INDÉTERMINÉ': 'MTB DETECTED VERY LOW||RIF Resistance INDETERMINATE',\n",
    "\t'MTB DÉTECTÉ ÉLEVÉ||RIF Resistance INDÉTERMINÉ': 'MTB DETECTED HIGH||RIF Resistance INDETERMINATE',\n",
    "\t'MTB DÉTECTÉ MOYEN||RIF Resistance DÉTECTÉ' : 'MTB DETECTED MEDIUM||RIF Resistance DETECTED',\n",
    "\t'MTB DÉTECTÉ BAS||RIF Resistance INDÉTERMINÉ' : 'MTB DETECTED LOW||RIF Resistance INDETERMINATE',\n",
    "\n",
    "\n",
    "\n",
    "\t'NON VALIDE' : 'INVALID',\n",
    "    'ERREUR' : 'ERROR',\n",
    "\t'PAS DE RÉSULTAT' : 'NO RESULT',\n",
    "    '|MTB Trace DETECTED|RIF Resistance INDETERMINATE' : 'MTB Trace DETECTED|RIF Resistance INDETERMINATE',\n",
    "    '|MTB Trace DÉTECTÉ|RIF Resistance INDÉTERMINÉ': 'MTB Trace DETECTED|RIF Resistance INDETERMINATE',\n",
    "\t'LS ' : '',\n",
    "\t',LS ': '',\n",
    "\t' LS' : '',\n",
    "\t'LS' : '',\n",
    "\t'LS:' : '',\n",
    "\t'CRACHAT' : 'SPUTUM',\n",
    "\t\"Nom de l'utilisateur du rapport\" : 'Report User Name', \n",
    "\t'Contrôle de sonde 1' : 'Prb Chk 1',\n",
    "\t'Contrôle de sonde 2' : 'Prb Chk 2',\n",
    "\t'Contrôle de sonde 3' : 'Prb Chk 3',\n",
    "\t'Résultat du contrôle de la sonde' : 'Probe Check Result',\n",
    "\t'Dérivée seconde  de la hauteur  du pic' : '2nd Deriv Peak Height',\n",
    "\t'Ajustement de courbe' : 'Curve Fit',\n",
    "\t'Température pic de fusion' : 'Melt Peak Temperature',\n",
    "\t'Hauteur pic de fusion' : 'Melt Peak Height',\n",
    "\t'Diagnostic in vitro' : 'In Vitro Diagnostic',\n",
    "\t\"Échantillon\" : 'Specimen',\n",
    "\t\"RÉUSSITE\" : 'PASS',\n",
    "\t\"ÉCHEC\" : 'FAIL',\n",
    "\t\"Nom de l'analyte\" : 'Analyte Name',\n",
    "\t'SO,' : 'NA,',\n",
    "\t\"SO\" : 'NA',\n",
    "    \"RÉUSSITE\" : 'PASS',\n",
    "    \n",
    "}\n",
    "\n",
    "# define the folder directory\n",
    "base_directory = \"E:\\\\CHPR_ORG\\\\Bamenda Center for Health Promotion and Research\\\\Data Management - XPERT EXPORT PROJECT\\\\Prospective Xpert Exports\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS ENGLISH FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnicodeDecodeError encountered while processing file: E:\\CHPR_ORG\\Bamenda Center for Health Promotion and Research\\Data Management - XPERT EXPORT PROJECT\\Prospective Xpert Exports\\TBRL Bamenda\\TBRL BDA- XPERT DATA 1-12-23 TO 4-12-23.csv\n",
      "UnicodeDecodeError encountered while processing file: E:\\CHPR_ORG\\Bamenda Center for Health Promotion and Research\\Data Management - XPERT EXPORT PROJECT\\Prospective Xpert Exports\\TBRL Bamenda\\TBRL BDA-XPERT DATA 11-11-23 TO 13-11-23.csv\n",
      "Data successfully extracted from 176 files, resulting in 6926 rows and 38 columns.\n"
     ]
    }
   ],
   "source": [
    "# Function to detect the encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return chardet.detect(f.read())['encoding']\n",
    "\n",
    "# Updated function for extraction focusing on the Detail section\n",
    "def refined_extraction_with_detail(file_path, columns_of_interest, detail_columns, encoding):\n",
    "    within_result_table = False\n",
    "    within_detail_section = False\n",
    "    processed_columns = set()\n",
    "    detail_columns_found = set()\n",
    "\n",
    "    with open(file_path, encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if \"RESULT TABLE\" in line:\n",
    "                processed_columns = set()\n",
    "                within_detail_section = False\n",
    "\n",
    "            if line in [\"<Insufficient privilege to access data>\", \"\\ufeff\"]:\n",
    "                continue\n",
    "\n",
    "            for header in metadata_sections + [\"RESULT TABLE\", \"Detail\"]:\n",
    "                if header in line:\n",
    "                    if header == \"RESULT TABLE\":\n",
    "                        within_result_table = True\n",
    "                        yield {}\n",
    "                    elif header == \"Detail\":\n",
    "                        within_detail_section = True\n",
    "                        detail_columns_found.clear()\n",
    "                    elif header in metadata_sections:\n",
    "                        within_result_table = False\n",
    "                    break\n",
    "\n",
    "            if within_detail_section and line.startswith(tuple(detail_columns)) and \",\" in line:\n",
    "                key, value = line.split(\",\", 1)\n",
    "                key = key.strip()\n",
    "                if key in detail_columns_found:\n",
    "                    continue\n",
    "                detail_columns_found.add(key)\n",
    "                yield {key: value.strip()}\n",
    "\n",
    "            elif within_result_table and not within_detail_section:\n",
    "                if line.startswith(tuple(columns_of_interest)) and \",\" in line:\n",
    "                    key, value = line.split(\",\", 1)\n",
    "                    key = key.strip()\n",
    "                    if key in processed_columns:\n",
    "                        continue\n",
    "                    processed_columns.add(key)\n",
    "                    yield {key: value.strip()}\n",
    "\n",
    "# Function to get all CSV files in a directory and its subdirectories\n",
    "def get_all_csv_files(directory):\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "# Process file function\n",
    "def process_file(file_path, columns_of_interest, detail_columns):\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        patient_data = {}\n",
    "        for extracted_data in refined_extraction_with_detail(file_path, columns_of_interest, detail_columns, encoding):\n",
    "            if not extracted_data:\n",
    "                if patient_data:\n",
    "                    patient_data[\"file\"] = os.path.basename(file_path)\n",
    "                    all_df_rows.append(patient_data)\n",
    "                    patient_data = {}\n",
    "            else:\n",
    "                patient_data.update(extracted_data)\n",
    "        if patient_data:\n",
    "            patient_data[\"file\"] = os.path.basename(file_path)\n",
    "            all_df_rows.append(patient_data)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError encountered while processing file: {file_path}\")\n",
    "\n",
    "# List of metadata sections to ignore\n",
    "metadata_sections = [\"GeneXpert Dx System\", \"ASSAY INFORMATION\", \"Analysis Settings\"]\n",
    "\n",
    "# Define the folder directory\n",
    "base_directory = \"E:\\\\CHPR_ORG\\\\Bamenda Center for Health Promotion and Research\\\\Data Management - XPERT EXPORT PROJECT\\\\Prospective Xpert Exports\"\n",
    "\n",
    "# List of columns of interest\n",
    "columns_of_interest = [\n",
    "    \"Cartridge S/N\", \"Module Name\", \"Module S/N\", \n",
    "    \"Instrument S/N\", \"S/W Version\", \"Reagent Lot ID\", \"Expiration Date\", \n",
    "    \"Start Time\", \"End Time\", \"Error Status\", \"Status\", \"User\", \n",
    "    \"Sample ID\", \"Patient ID\", \"Assay\", \"Assay Version\", \n",
    "    \"Test Type\", \"Sample Type\", \"Notes\", \"Test Result\"\n",
    "]\n",
    "\n",
    "# Additional columns to be extracted from the Detail section\n",
    "detail_columns = [\"SPC\", \"IS1081-IS6110\", \"rpoB1\", \"rpoB2\", \"rpoB3\", \"rpoB4\"]\n",
    "\n",
    "# Initialize a list to store all extracted rows\n",
    "all_df_rows = []\n",
    "\n",
    "# Process all files in the directory and its subdirectories\n",
    "csv_files = get_all_csv_files(base_directory)\n",
    "for file_path in csv_files:\n",
    "    process_file(file_path, columns_of_interest, detail_columns)\n",
    "\n",
    "# Create the final DataFrame\n",
    "dfe = pd.DataFrame(all_df_rows)\n",
    "print(f\"Data successfully extracted from {len(csv_files)} files, resulting in {dfe.shape[0]} rows and {dfe.shape[1]} columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS THE FRENCH FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnicodeDecodeError encountered while processing file: E:\\CHPR_ORG\\Bamenda Center for Health Promotion and Research\\Data Management - XPERT EXPORT PROJECT\\Prospective Xpert Exports\\TBRL Bamenda\\TBRL BDA- XPERT DATA 1-12-23 TO 4-12-23.csv\n",
      "UnicodeDecodeError encountered while processing file: E:\\CHPR_ORG\\Bamenda Center for Health Promotion and Research\\Data Management - XPERT EXPORT PROJECT\\Prospective Xpert Exports\\TBRL Bamenda\\TBRL BDA-XPERT DATA 11-11-23 TO 13-11-23.csv\n",
      "Data successfully extracted from 176 files, resulting in 3102 rows and 36 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Function to detect the encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return chardet.detect(f.read())['encoding']\n",
    "\n",
    "# Function for extraction focusing on the Detail section\n",
    "def refined_extraction_with_detail(file_path, columns_of_interest, detail_columns, encoding):\n",
    "    within_result_table = False\n",
    "    within_detail_section = False\n",
    "    processed_columns = set()\n",
    "    detail_columns_found = set()\n",
    "\n",
    "    with open(file_path, encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if \"TABLEAU DE RÉSULTATS\" in line:\n",
    "                processed_columns = set()\n",
    "                within_detail_section = False\n",
    "\n",
    "            if line in [\"<Privilège insuffisant pour accéder aux données>\", \"\\ufeff\"]:\n",
    "                continue\n",
    "\n",
    "            for header in metadata_sections + [\"TABLEAU DE RÉSULTATS\"]:\n",
    "                if header in line:\n",
    "                    if header == \"TABLEAU DE RÉSULTATS\":\n",
    "                        within_result_table = True\n",
    "                        yield {}\n",
    "                    elif header in metadata_sections:\n",
    "                        within_result_table = False\n",
    "                    break\n",
    "\n",
    "            if \"Détail\" in line:\n",
    "                within_detail_section = True\n",
    "                detail_columns_found.clear()\n",
    "                continue\n",
    "\n",
    "            if within_detail_section and line.startswith(tuple(detail_columns)) and \",\" in line:\n",
    "                key, value = line.split(\",\", 1)\n",
    "                key = key.strip()\n",
    "                if key in detail_columns_found:\n",
    "                    continue\n",
    "                detail_columns_found.add(key)\n",
    "                yield {key: value.strip()}\n",
    "\n",
    "            elif within_result_table and not within_detail_section:\n",
    "                if line.startswith(tuple(columns_of_interest)) and \",\" in line:\n",
    "                    key, value = line.split(\",\", 1)\n",
    "                    key = key.strip()\n",
    "                    if key in processed_columns:\n",
    "                        continue\n",
    "                    processed_columns.add(key)\n",
    "                    yield {key: value.strip()}\n",
    "\n",
    "# Function to get all CSV files in a directory and its subdirectories\n",
    "def get_all_csv_files(directory):\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "# Process file function\n",
    "def process_file(file_path, columns_of_interest, detail_columns):\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        patient_data = {}\n",
    "        for extracted_data in refined_extraction_with_detail(file_path, columns_of_interest, detail_columns, encoding):\n",
    "            if not extracted_data:\n",
    "                if patient_data:\n",
    "                    patient_data[\"file\"] = os.path.basename(file_path)\n",
    "                    all_df_rows.append(patient_data)\n",
    "                    patient_data = {}\n",
    "            else:\n",
    "                patient_data.update(extracted_data)\n",
    "        if patient_data:\n",
    "            patient_data[\"file\"] = os.path.basename(file_path)\n",
    "            all_df_rows.append(patient_data)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError encountered while processing file: {file_path}\")\n",
    "\n",
    "# List of metadata sections to ignore\n",
    "metadata_sections = [\"GeneXpert Dx System\", \"INFORMATIONS SUR LE TEST\", \"Paramètres d'analyse\"]\n",
    "\n",
    "# Define the folder directory\n",
    "base_directory = \"E:\\\\CHPR_ORG\\\\Bamenda Center for Health Promotion and Research\\\\Data Management - XPERT EXPORT PROJECT\\\\Prospective Xpert Exports\"\n",
    "\n",
    "# List of columns of interest\n",
    "columns_of_interest = ['Numéro de série de la cartouche', 'Nom du module', 'N° de série du module', \"N° de série de l'instrument\",\n",
    "                        'Version du logiciel', 'N° du lot',  \"Date d'expiration\", 'Heure de lancement',  'Heure de fin',  \"État de l'erreur\",\n",
    "                        'État', 'Utilisateur',  \"N° Id de l'échantillon\",  'N° Id du patient',  'Test',  'Version du test',  'Type de test',\n",
    "                        \"Type d'échantillon\",  'Remarques',  'Résultat du test']\n",
    "\n",
    "# Additional columns to be extracted from the Detail section\n",
    "detail_columns = [\"SPC\", \"IS1081-IS6110\", \"rpoB1\", \"rpoB2\", \"rpoB3\", \"rpoB4\"]\n",
    "\n",
    "# Initialize a list to store all extracted rows\n",
    "all_df_rows = []\n",
    "\n",
    "# Process all files in the directory and its subdirectories\n",
    "csv_files = get_all_csv_files(base_directory)\n",
    "for file_path in csv_files:\n",
    "    process_file(file_path, columns_of_interest, detail_columns)\n",
    "\n",
    "# Create the final DataFrame\n",
    "dff = pd.DataFrame(all_df_rows)\n",
    "print(f\"Data successfully extracted from {len(csv_files)} files, resulting in {dff.shape[0]} rows and {dff.shape[1]} columns.\")\n",
    "\n",
    "# Assuming language_map is a dictionary for column renaming\n",
    "dff.rename(columns=language_map, inplace=True)\n",
    "dff.replace(language_map, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE AND PROCESS XPERT FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lists, Dictionaries for replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define list of Pool IDs in Patient ID to be Replaced in Sample ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id_list = [\n",
    "    \"P17 1-1-2-2\",\n",
    "    \"P15 5-5\",\n",
    "    \"P16 6-6\",\n",
    "    \"P11 1-1-2\",\n",
    "    \"P13 3-3\",\n",
    "    \"P14 4-4\",\n",
    "    \"P12 6-6\",\n",
    "    \"P18 7-8\",\n",
    "    \"POO2- 2-2\",\n",
    "    \"P0001-1-3-8\",\n",
    "    \"POOL-3 8-1-2\",\n",
    "    'P688-1-2-3',\n",
    "    'P14  4-4',\n",
    "    \"POOl-3 8-1-2\",\n",
    "    'P0081-9-2-1-2',\n",
    "    'P-000041-88-89-90',\n",
    "    'P000040-85-86-87',\n",
    "    'P000030-55-56-57',\n",
    "    'P000029-52-53-54',\n",
    "    'P000028-49-50-51',\n",
    "    'P000027-46-47-48',\n",
    "    'P000026-43-44-45',\n",
    "    'P000025-40-41-42',\n",
    "    'P000024-37-38-39',\n",
    "    'P000024-37-38-39',\n",
    "    'P000024-37-38-39',\n",
    "    'P0057- 7-6-7-5 F 99',\n",
    "    'P0056- 8-0-5-8 F 99',\n",
    "    'POO55 6-2-5-3 F 99',\n",
    "    'POO54 0-8-7-5 F 99',\n",
    "    'POO53 6-9-5-3 F 99',\n",
    "    'POO52 5-2-4-6 F',\n",
    "    'POO49 2-3-9-2 F 99',\n",
    "    'POO48 4-6-0-1 F 99',\n",
    "    'POO51- 1-3-5-8 F 99',\n",
    "    'P0050- 3-6-7-8 F 99',\n",
    "    'POOL44 12-18-19-20 F 99',\n",
    "    'POOL44 12-18-19-20 F',\n",
    "    'POOL43 30-31-32-33 F 99',\n",
    "    'POOL34 02-03-04-05 F 99',\n",
    "    'POOL42 2699-2700 F 99',\n",
    "    'POO67- 3-4 F 99',\n",
    "    'P0062- 5-6-7-8 F 99',\n",
    "    'POO 65 6-6-8 F 99',\n",
    "    'POO63- 6-5-8 F 99',\n",
    "    'POO61 1-7-4 F 99',\n",
    "    'POO60 4-2-3 F 99',\n",
    "    'POOL37 3-7-4-5 F 99',\n",
    "    'POOL 36 60-62-63-64 F 99',\n",
    "    'POOL35 3-7-58-110 F 99',\n",
    "    'POOL33 09-22-52-111 F 99',\n",
    "    'POOL34 24-49-50-51 F 99',\n",
    "    'POOL32 108-107 F 99',\n",
    "    'POOL31 103-106 F 99',\n",
    "    'POOL30 102-17-46 F 99',\n",
    "    'POOL29 99-101 F 99',\n",
    "    'POOL28 87-89 F 99',\n",
    "    'POOL 27 2-84-85-86 F 99',\n",
    "    'POOL25 69-72-81-83 F 99',\n",
    "    'POOL24 81-83-72-69 F 99',\n",
    "    'POOL23 12-82-48-67 F 99',\n",
    "    'POOL22 51-11-13-70 F 99',\n",
    "    'POOL21 75-80-77-71 F 99',\n",
    "    'POOL20 65-43-42 F 99',\n",
    "    'POOL5 2-2-1 F 99',\n",
    "    'POOL Esther.David.Tchoupoulo F 9',\n",
    "    'POOL19 68-74-66 F 99',\n",
    "    'POOL18 78-76-73-75 F 99',\n",
    "    'POOL17 5-7-47 F 99',\n",
    "    'POOL16 43-44-45-46 F 99',\n",
    "    'POOL15 8-10-36-41 F 99',\n",
    "    'POOL14 27-33-34-38 F 99',\n",
    "    'POOL13 32-35-37-38 F 99',\n",
    "    'POOL12 24--41-31-50 F 99',\n",
    "    'POOL12 24-31-41-50 F 99',\n",
    "    'POOL11 25-26-28 F 99',\n",
    "    'POOL11 25-26-28 F 99',\n",
    "    'POOL11 25-26-28 F 99',\n",
    "    'POOL10 21-23-39 F 99',\n",
    "    'POOL9 1-6-20 F 99',\n",
    "    'POOL8 4-18-17-3 F 99',\n",
    "    'POOL7 11-13-14-16 F',\n",
    "    'POOL6 12-15 F 99',\n",
    "    'POO2 1-2-3-37 F 99',\n",
    "    'P040-5-6-7',\n",
    "    'P030-5-6-7',\n",
    "    'P029-2-3-4',\n",
    "    'P028-9-0-1',\n",
    "    'P027-6-7-8',\n",
    "    'P026-3-4-5',\n",
    "    'P025-0-1-2',\n",
    "    \"p0058- 9-3-6-4 F 99\",\n",
    "    \"P0006-7-8\",\n",
    "    \"P0003-4-5\",\n",
    "    \"P0022-0-3-1-7-7\"\n",
    "    \n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary of Pool IDs to be replaceed with the Correct Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of Pool IDs to repalce\n",
    "pool_id_dict = {\n",
    "    'P0057- 7-6-7-5 F 99' : 'P0057-7-6-7-5',\n",
    "    'P0056- 8-0-5-8 F 99' : 'P0056-8-0-5-8',\n",
    "    'POO55 6-2-5-3 F 99' : 'POO55-6-2-5-3',\n",
    "    'POO54 0-8-7-5 F 99' : 'POO54-0-8-7-5',\n",
    "    'POO53 6-9-5-3 F 99' : 'POO53-6-9-5-3',\n",
    "    'POO52 5-2-4-6 F' : 'POO52-5-2-4-6',\n",
    "    'POO49 2-3-9-2 F 99' : 'POO49-2-3-9-2',\n",
    "    'POO48 4-6-0-1 F 99' : 'POO48-4-6-0-1',\n",
    "    'POO51- 1-3-5-8 F 99' : 'POO51-1-3-5-8',\n",
    "    'P0050- 3-6-7-8 F 99' : 'P0050-3-6-7-8',\n",
    "    'POOL44 12-18-19-20 F 99' : 'POOL44-12-18-19-20',\n",
    "    'POOL44 12-18-19-20 F' : 'POOL44-12-18-19-20',\n",
    "    'POOL43 30-31-32-33 F 99' : 'POOL43-30-31-32-33',\n",
    "    'POOL34 02-03-04-05 F 99' : 'POOL34-02-03-04-05',\n",
    "    'POOL42 2699-2700 F 99' : 'POOL42-2699-2700',\n",
    "    'POO67- 3-4 F 99' : 'POO67-3-4',\n",
    "    'P0062- 5-6-7-8 F 99' : 'P0062-5-6-7-8',\n",
    "    'POO 65 6-6-8 F 99' : 'POO-65-6-6-8',\n",
    "    'POO63- 6-5-8 F 99' : 'POO63-6-5-8',\n",
    "    'POO61 1-7-4 F 99' : 'POO61-1-7-4',\n",
    "    'POO60 4-2-3 F 99' : 'POO60-4-2-3',\n",
    "    'POOL37 3-7-4-5 F 99' : 'POOL37-3-7-4-5',\n",
    "    'POOL 36 60-62-63-64 F 99' : 'POOL-36-60-62-63-64',\n",
    "    'POOL35 3-7-58-110 F 99' : 'POOL35-3-7-58-110',\n",
    "    'POOL33 09-22-52-111 F 99' : 'POOL33-09-22-52-111',\n",
    "    'POOL34 24-49-50-51 F 99' : 'POOL34-24-49-50-51',\n",
    "    'POOL32 108-107 F 99' : 'POOL32-108-107',\n",
    "    'POOL31 103-106 F 99' : 'POOL31-103-106',\n",
    "    'POOL30 102-17-46 F 99' : 'POOL30-102-17-46',\n",
    "    'POOL29 99-101 F 99' : 'POOL29-99-101',\n",
    "    'POOL28 87-89 F 99' : 'POOL28-87-89',\n",
    "    'POOL 27 2-84-85-86 F 99' : 'POOL-27-2-84-85-86',\n",
    "    'POOL25 69-72-81-83 F 99' : 'POOL25-69-72-81-83',\n",
    "    'POOL24 81-83-72-69 F 99' : 'POOL24-81-83-72-69',\n",
    "    'POOL23 12-82-48-67 F 99' : 'POOL23-12-82-48-67',\n",
    "    'POOL22 51-11-13-70 F 99' : 'POOL22-51-11-13-70',\n",
    "    'POOL21 75-80-77-71 F 99' : 'POOL21-75-80-77-71',\n",
    "    'POOL20 65-43-42 F 99' : 'POOL20-65-43-42',\n",
    "    'POOL5 2-2-1 F 99' : 'POOL5-2-2-1',\n",
    "    'POOL Esther.David.Tchoupoulo F 9' : 'POOL23-33-34-35',\n",
    "    'POOL19 68-74-66 F 99' : 'POOL19-68-74-66',\n",
    "    'POOL18 78-76-73-75 F 99' : 'POOL18-78-76-73-75',\n",
    "    'POOL17 5-7-47 F 99' : 'POOL17-5-7-47',\n",
    "    'POOL16 43-44-45-46 F 99' : 'POOL16-43-44-45-46',\n",
    "    'POOL15 8-10-36-41 F 99' : 'POOL15-8-10-36-41',\n",
    "    'POOL14 27-33-34-38 F 99' : 'POOL14-27-33-34-38',\n",
    "    'POOL13 32-35-37-38 F 99' : 'POOL13-32-35-37-38',\n",
    "    'POOL12 24--41-31-50 F 99' : 'POOL12-24-41-31-50',\n",
    "    'POOL12 24-31-41-50 F 99' : 'POOL12-24-31-41-50',\n",
    "    'POOL11 25-26-28 F 99' : 'POOL11-25-26-28',\n",
    "    'POOL11 25-26-28 F 99' : 'POOL11-25-26-28',\n",
    "    'POOL11 25-26-28 F 99' : 'POOL11-25-26-28',\n",
    "    'POOL10 21-23-39 F 99' : 'POOL10-21-23-39',\n",
    "    'POOL9 1-6-20 F 99' : 'POOL9-1-6-20',\n",
    "    'POOL8 4-18-17-3 F 99' : 'POOL8-4-18-17-3',\n",
    "    'POOL7 11-13-14-16 F' : 'POOL7-11-13-14-16',\n",
    "    'POOL6 12-15 F 99' : 'POOL6-12-15',\n",
    "    'POO2 1-2-3-37 F 99' : 'POO2-1-2-3-37',\n",
    "    'P-000041-88-89-90' : 'P000041-88-89-90',\n",
    "    \"4BRH-01749-NA-NEW-HOS-SPU\" : \"P000041-88-89-90-80-80-80-80\",\n",
    "    \"6MBI-00301-AH\" : \"P000041-8-8-8-8\",\n",
    "    \"p0058- 9-3-6-4 F 99\" : \"P0058-9-3-6-4\",\n",
    "    \"1MOK000095\" : \"1MOK-00095\",\n",
    "    \"1MOK000094\" : \"1MOK-00094\",\n",
    "    \"1MOK0000180\" : \"1MOK-00180\",\n",
    "    '2' : 'P000041-88-89-72',\n",
    "    \"21\" : \"P000041-83-99-71\",\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a dictionary of Clean Notes to be replaced with the correct counterpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a dictionary to further clean the notes field\n",
    "clean_notes_replacement_dict = {\"KB.PP.SM.YC P1711-8-0-1-2 LS-120058-KB-2 LS-120060-PP-1 LS-120061-SM-1 LS-120062-YC-1\": \"LS-120058-KB-2 LS-120060-PP-1 LS-120061-SM-1 LS-120062-YC-1\",\n",
    "    \"POOL NJ.AG.BA.BA P1710-4-5-9-9 LS-120054-NJ-2 LS-120055-AG-2 LS-120059-BA-1 LS-120059-BA-2\" : \"LS-120054-NJ-2 LS-120055-AG-2 LS-120059-BA-1 LS-120059-BA-2\",\n",
    "    \"4BRH-00419-S 4BRH-00417-TG 4BRH-00354-4BRH-00420-OB\": \"4BRH-00419-S 4BRH-00417-TG 4BRH-00354 4BRH-00420-OB\",\n",
    "    \"1 MOR-00039-A\": \"1MOR-00039-A\",\n",
    "    \"2ESP-00234 2ESP-00235-2ESP-00237\" : \"2ESP-00234 2ESP-00235 2ESP-00237\",\n",
    "    \"2ESP-00234 2ESP-00235-2ESP-00237\" : \"2ESP-00234 2ESP-00235 2ESP-00237\",\n",
    "    \"KB.PP.SM.YC P1711-8-0-1-2 LS-120058-KB-2 LS-120060-PP-1 LS-120061-SM-1 LS-120062-YC-1\" : \"LS-120058-KB-2 LS-120060-PP-1 LS-120061-SM-1 LS-120062-YC-1\",\n",
    "    \"POOL NJ.AG.BA.BA P1710-4-5-9-9 LS-120054-NJ-2 LS-120055-AG-2 LS-120059-BA-1 LS-120059-BA-2\" : \"LS-120054-NJ-2 LS-120055-AG-2 LS-120059-BA-1 LS-120059-BA-2\",\n",
    "    \"4BRH-00419-S 4BRH-00417-TG 4BRH-00354-4BRH-00420-OB\": \"4BRH-00419-S 4BRH-00417-TG 4BRH-00354 4BRH-00420-OB\",\n",
    "    \"101 10113-YM 10115-AB 10116-SS 10110-TE\": \"10113-YM 10115-AB 10116-SS 10110-TE\",\n",
    "    \"00419-S 00417-TG 00354-00420-OB\": \"00419-S 00417-TG 00354 00420-OB\",\n",
    "    \"4SJC-00002-JL 4SJC-00003- MA 4SJC-00004-ER 4SJC-00005-TC\": \"4SJC-00002-JL 4SJC-00003-MA 4SJC-00004-ER 4SJC-00005-TC\",\n",
    "    \"126329-126330-AJ 126331-NR 126332-BB\": \"126329 126330-AJ 126331-NR 126332-BB\",\n",
    "    \"6MBI-00442-6MMI-00443-BA\": \"6MBI-00442 6MMI-00443-BA\",\n",
    "    \"6MBP-01805-MN 6MBP-01806-6MBP-01807-WZ 6MBP-01808-NA\": \"6MBP-01805-MN 6MBP-01806 6MBP-01807-WZ 6MBP-01808-NA\",\n",
    "    \"6MBP-01221-6MBP-01222-MM 6MBP-01223-NN 6MBP-01224-ND\" : \"6MBP-01221 6MBP-01222-MM 6MBP-01223-NN 6MBP-01224-ND\",\n",
    "    \"6MBI-00442-6MMI-00443-BA\": \"6MBI-00442 6MMI-00443-BA\",\n",
    "    \"6MBP-01805-MN 6MBP-01806-6MBP-01807-WZ 6MBP-01808-NA\": \"6MBP-01805-MN 6MBP-01806 6MBP-01807-WZ 6MBP-01808-NA\",\n",
    "    \"6MBP-02559-TC 6MBP-02561-6MBP-02562-NL 6MBP-02563-NE\": \"6MBP-02559-TC 6MBP-02561 6MBP-02562-NL 6MBP-02563-NE\",\n",
    "    \"1DAR-00091-IM 1DJ-00661-1DJK-0034-FN 1DAR-00083-MA\": \"1DAR-00091-IM 1DJ-00661 1DJK-0034-FN 1DAR-00083-MA\",\n",
    "    \"1DAR-00068-TYB1DAR-00067-DY 1DAR-00069-ST 1DAR-00070-S0 1FOU-00698-AB 1FOU-00635-OA 1FOU-00696-HB 1FOU-00700-MH\": \"1DAR-00068-TYB 1DAR-00067-DY 1DAR-00069-ST 1DAR-00070-S0 1FOU-00698-AB 1FOU-00635-OA 1FOU-00696-HB 1FOU-00700-MH\",\n",
    "    \"1DJK-00569-ST 1DJK-00568-FA 1DJK-00567-DA 1DJK-00566- SI 1DJK-00575-LM\": \"1DJK-00569-ST 1DJK-00568-FA 1DJK-00567-DA 1DJK-00566-SI 1DJK-00575-LM\",\n",
    "    \"62070-YS-SPU-1 62073-AC- SPU-1 62099-RS- SPU-1\" :  \"62070-YS-SPU-1 62073-AC-SPU-1 62099-RS-SPU-1\",\n",
    "    \"HAOUA HAMADOU-320-HH-SPU-FOUN MAIRAOU BOUBA-321-MB-SPU-FOUN\": \"320-HH-SPU-FOUN 321-MB-SPU-FOUN\",\n",
    "    \"11541-DM-SPU-1 L 11539-GM-SPU-1\" : \"11541-DM-SPU-1 11539-GM-SPU-1\",\n",
    "    \"PS 10113-BE-SPU-1 PS10112-DB-SPU-1\" : \"10113-BE-SPU-1 10112-DB-SPU-1\",\n",
    "    \"101111-AM-SPU- 1 101114-FM -SPU-1 101110 -AA-SPU-1\" : \"101111-AM-SPU-1 101114-FM-SPU-1 101110-AA-SPU-1\",\n",
    "    \"11394-AS -SPU-1 11395-AS-SPU-1 11396-NL-SPU-1 11398-OH-SPU-1\" : \"11394-AS-SPU-1 11395-AS-SPU-1 11396-NL-SPU-1 11398-OH-SPU-1\",\n",
    "    \"HAMADOU SOULEY A et B\" :\"\",\n",
    "    \"61493-PS-1SPU-00001\": \"61493-PS-1 SPU-00001\",\n",
    "    \"1 P0001-1DER-00033-BT 2 P0001-1DER-00034-GE 3 P0001-1DER-00035-BD\" : \"1DER-00033-BT 1DER-00034-GE 1DER-00035-BD\",\n",
    "    '1MOK-000088-VV-1MOK-000089-NN-1MOK-000090-CC' : '1MOK-00088-VV- 1MOK-00089-NN- 1MOK-00090-CC',\n",
    "    '1MOK-000085-DD-1MOK-000086-CC-1MOK-000087-KC' : '1MOK-00085-DD- 1MOK-00086-CC- 1MOK-00087-KC',\n",
    "    '1MOK-000055-HH-1MOK-000056-NL-1MOK-000057-NN' : '1MOK-00055-HH- 1MOK-00056-NL- 1MOK-00057-NN',\n",
    "    '1MOK-000052-HB-1MOK-000053-VG-1MOK-000054-BD' : '1MOK-00052-HB- 1MOK-00053-VG- 1MOK-00054-BD',\n",
    "    '1MOK-000049-FJ-1MOK-000050-HH-1MOK-000051-SA' : '1MOK-00049-FJ- 1MOK-00050-HH- 1MOK-00051-SA',\n",
    "    '1MOK-000046-MM-1MOK-000047-FF-1MOK-0000-48-AA' : '1MOK-00046-MM- 1MOK-00047-FF- 1MOK-00048-AA',\n",
    "    '1MOK-000043-ZM-1MOK-000044-HD-1MOK-000045-RH' : '1MOK-00043-ZM- 1MOK-00044-HD- 1MOK-00045-RH',\n",
    "    '1MOK-000040-EM-1MOK-000041-FK-1MOK-000042-MM' : '1MOK-00040-EM- 1MOK-00041-FK- 1MOK-00042-MM',\n",
    "    '1MOK-000037KB-1MOK-000038LJ-1MOK-000039HA' : '1MOK-00037KB- 1MOK-00038LJ- 1MOK-00039HA',\n",
    "    '1MOK0000-34-KW-1MOK0000-35-LI-1MKL0000-36-GA' : '1MOK-00034-KW- 1MOK-00035-LI- 1MKL-00036-GA',\n",
    "    '1MOK000-9-MT-1MOK000-10-WS-1MOK000-11-DG' : '1MOK-00009-MT- 1MOK-00010-WS- 1MOK-00011-DG',\n",
    "    '1MOK000-15-AG-1MOK000-16-AO-1MOK000-17-YR' : '1MOK-00015-AG- 1MOK-00016-AO- 1MOK-00017-YR',\n",
    "    '1MOK-000037KB-1MOK-000038LJ-1MOK-000039HA' : '1MOK-00037KB- 1MOK-00038LJ- 1MOK-00039HA',\n",
    "    '1MOK000-15-AG-1MOK000-16-AO-1MOK000-17-YR' : '1MOK-00015-AG- 1MOK-00016-AO- 1MOK-00017-YR',\n",
    "    '1MOK-000037KB-1MOK-000038LJ-1MOK-000039HA' : '1MOK-00037KB- 1MOK-00038LJ- 1MOK-00039HA',\n",
    "    '1MOK000-15-AG-1MOK000-16-AO-1MOK000-17-YR' : '1MOK-00015-AG- 1MOK-00016-AO- 1MOK-00017-YR',\n",
    "    \"1MOK-00049FJ 1MOK-00050-HH-1MOK-00051-SA\" : \"1MOK-00049FJ 1MOK-00050-HH- 1MOK-00051-SA\",\n",
    "    \"1MOK-00049FJ 1MOK-00050-HH-1MOK-00051-SA\" : \"1MOK-00049FJ 1MOK-00050-HH- 1MOK-00051-SA\",\n",
    "\n",
    "\n",
    "\n",
    "    '6MBI-00442-LS 6MMI-00443-BA' : '6MBI-00442-LS 6MBI-00443-BA',\n",
    "    '6MBI-00442-LS 6MMI-00443-BA' : '6MBI-00442-LS 6MBI-00443-BA',\n",
    "    'POO55-1YAG-00129NT-1YAG-00135GC-1YAG-00137KC-1YAG-00114HR' : '1YAG-00129NT- 1YAG-00135GC- 1YAG-00137KC- 1YAG-00114HR',\n",
    "    'POO54-1YAG-00120HI-1YAG-00127DA-1YAG-00114HC-1YAG-00119VB' : '1YAG-00120HI- 1YAG-00127DA- 1YAG-00114HC- 1YAG-00119VB',\n",
    "    'POO53-1YAG-00151-TN-1YAG-00149TD-1YAG-00152NA-1YAG-00150RO' : '1YAG-00151-TN- 1YAG-00149TD- 1YAG-00152NA- 1YAG-00150RO',\n",
    "    'POO67-1YAG-00213AL-1YAG00214GA' : '1YAG-00213AL- 1YAG00214GA',\n",
    "    'POO62-1DAN-00005GC-1DAN-00006NB-1DAN-00007MI-1DAN-00008HM' : '1DAN-0005GC- 1DAN-0006NB- 1DAN-0007MI- 1DAN-0008HM',\n",
    "    'POO61-2801-BG-2807-MA-2804-OM' : '2801-BG- 2807-MA- 2804-OM',\n",
    "    'POO60-1YAG-00162-FA-1YAG-00163-FB-1YAG-00164-GJ' : '1YAG-00162-FA- 1YAG-00163-FB- 1YAG-00164-GJ',\n",
    "    \"POO67-1YAG-00213AL-1YAG00214GA\" : \"1YAG-00213 1YAG-00214\",\n",
    "    \"POO67-1YAG-00213AL-1YAG-00214GA\" : \"1YAG-00213 1YAG-00214\",\n",
    "    \"POOL44-2712GORSOU\" : np.nan,\n",
    "    \"61493-PS-1SPU-1\" : np.nan,\n",
    "    \"CONTAMINATION CHECK INSTRUMENT A CONTAMINATION CHECK INSTRUMENT C SURFACE CONTAMINATION CHECK\" : np.nan,\n",
    "    \"CON CHECK A CON CHECK C SURFACE CON CHECK\" : np.nan,\n",
    "    \"5MUT-012 5MUT-012 5MUT-012 5MUT-01299-NB 5MUT-01300-MK 5MUT-01301-DM 5MUT-01302-AG\" : \"5MUT-01299-NB 5MUT-01300-MK 5MUT-01301-DM 5MUT-01302-AG\",\n",
    "    \"2GNA-00023-DS 2GNA-00010-2GNA-00024-MC\" : \"2GNA-00023-DS 2GNA-00010 2GNA-00024-MC\",\n",
    "    \"POOL43 2702-FADIMATOU-2703-LOGOSSAI-2704-FLEKKA-2705-YADI\" : \"2702 2703 2704 2705\",\n",
    "    \"4TDH-00005-TJ 4TDH-0005MUT-01408-DG 5MUT-01409-TE\" : \"4TDH-00005-TJ 4TDH-00006 5MUT-01408-DG 5MUT-01409-TE\",\n",
    "    \"4NHC-00055-AE 4NHC-00056-4NHC-00057-NH 4NHC-00058-NM\" : \"4NHC-00055-AE 4NHC-00056 4NHC-00057-NH 4NHC-00058-NM\",\n",
    "    \"2DJA-00846 2DJA-00863 2DJA-00600 2DJA-899\" : \"2DJA-00846 2DJA-00863 2DJA-00600 2DJA-00899\",\n",
    "    \"6MBP-02866MBP-02865-SS 6MBP-02866-TM 6MBP-02867-TS 6MBP-02868-NK\" : \"6MBP-02865-SS 6MBP-02866-TM 6MBP-02867-TS 6MBP-02868-NK\",\n",
    "    \"POOL44-2712-GORSOU ETIENNE-2718-ZARA KOURANA-2719-MAIGONWA MARTINE-2720-MAHAMAT\" : \"2712 2718 2719 2720\",\n",
    "    \"POO49 1YAG-00132LH 1YAG-00133MJ 1YAG-00139DG 1YAG-00142LC\" : \"1YAG-00132LH 1YAG-00133MJ 1YAG-00139DG 1YAG-00142LC\",\n",
    "    \"POOL43 1CYA-00030AISSATOU GAMBO1YCA-00031AWIKOUA EUGENIE1CYA-00032ADAMOU SOUALEY1CYA-00033ZANNOUBA\": \"1CYA-00030 1CYA-00031 1CYA-00032 1CYA-00033\",\n",
    "    \"Salomond paka MTB833 Wadja chikabou MTB827\" : \"MTB833 MTB827\",\n",
    "    \"POO52 1YAG-00115ME 1YAG-001122GU 1YAG-00124NC1YAG-00126MD\" : \"1YAG-00115ME 1YAG-001122GU 1YAG-00124NC 1YAG-00126MD\",\n",
    "    \"POOL42 2669-HAYATOU-2700-LITASSOU\" : \"2669 2700\",\n",
    "    \"POO 65-1YAG-00166-MC-1CYA-00036-RO-1CYA-00038-DS\": \"1YAG-00166-MC 1CYA-00036-RO 1CYA-00038-DS\",\n",
    "    \"KB.PP.SM.YC P1711-8-0-1-2 LS-120058-KB-2 LS-120060-PP-1 LS-120061-SM-1 LS-120062-YC-1\" : \"LS-120058-KB-2 LS-120060-PP-1 LS-120061-SM-1 LS-120062-YC-1\",\n",
    "    \"POOL NJ.AG.BA.BA P1710-4-5-9-9 LS-120054-NJ-2 LS-120055-AG-2 LS-120059-BA-1 LS-120059-BA-2\" : \"LS-120054-NJ-2 LS-120055-AG-2 LS-120059-BA-1 LS-120059-BA-2\",\n",
    "    \"KB-PP-SM-YC P1711-8-0-1-2 LS-120058-KB-2 LS-120060-PP-1 LS-120061-SM-1 LS-120062-YC-1\" : \"LS-120058-KB-2 LS-120060-PP-1 LS-120061-SM-1 LS-120062-YC-1\",\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qr_code_replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_code_replacement_dict = {\n",
    "\n",
    "    '1D1R-00018-GH' : '1DAR-00018',\n",
    "    '1DAR-00057-' : '1DAR-00057',\n",
    "    '1DJO0-00001-OA' : '1DJK-00001',\n",
    "    '1DJK-OO228' : '1DJK-00228',\n",
    "    '1DJK-OO230' : '1DJK-00230',\n",
    "    '1DJK-OO233' : '1DJK-00233',\n",
    "    '1DJ-00650-VV' : '1DJK-00650',\n",
    "    '1DJ-00661' : '1DJK-00661',\n",
    "    '1K1L-00009-YO' : '1KAL-00009',\n",
    "    '1KGAL-00073-VA' : '1KAL-00073',\n",
    "    '1KGAL-00074-DH' : '1KAL-00074',\n",
    "    '1KR-00003-HA' : '1KRG-00003',\n",
    "    '1KR-00003-HA' : '1KRG-00003',\n",
    "    '1MKL0000-58-WM' : '1MKL-00058',\n",
    "    '1MKL0000-59-AD' : '1MKL-00059',\n",
    "    '1MKL0000-60-VB' : '1MKL-00060',\n",
    "    '1MKL0000-62-YD' : '1MKL-00062',\n",
    "    '1MKL0000-63-DR' : '1MKL-00063',\n",
    "    '1MKL0000-64-AA' : '1MKL-00064',\n",
    "    '1MKL0000-65-AD' : '1MKL-00065',\n",
    "    '1MKL0000-66-MA' : '1MKL-00066',\n",
    "    '1MKL0000-67-DA' : '1MKL-00067',\n",
    "    '1MKL0000-68-FA' : '1MKL-00068',\n",
    "    '1MKL0000-69-AB' : '1MKL-00069',\n",
    "    '1MKL0000-70-AD' : '1MKL-00070',\n",
    "    '1MKL0000-71-TA' : '1MKL-00071',\n",
    "    '1MKL0000-72-BD' : '1MKL-00072',\n",
    "    '1MKL0000-73-FJ' : '1MKL-00073',\n",
    "    '1MKL0000-74-GB' : '1MKL-00074',\n",
    "    '1MKL0000-75NT' : '1MKL-00075',\n",
    "    '1MKL0000-76-MR' : '1MKL-00076',\n",
    "    '1MKL0000-77-AA' : '1MKL-00077',\n",
    "    '1MKL0000-78-AB' : '1MKL-00078',\n",
    "    '1MKL0000-79-DG' : '1MKL-00079',\n",
    "    '1MKL0000-80-DW' : '1MKL-00080',\n",
    "    '1MKL0000-81-JA' : '1MKL-00081',\n",
    "    '1MOK0009-MT' : '1MOK-00009',\n",
    "    '1MOK0000-24-MM' : '1MOK-00024',\n",
    "    '1MOK0000-25-WD' : '1MOK-00025',\n",
    "    '1MOK0000-26-MS' : '1MOK-00026',\n",
    "    '1MOK0000-31-D0' : '1MOK-00031',\n",
    "    '1MOK0000-32-DC' : '1MOK-00032',\n",
    "    '1MOK0000-33-GK' : '1MOK-00033',\n",
    "    '1MOK0043-ZM' : '1MOK-00043',\n",
    "    '1MOK0000-73-FJ' : '1MOK-00073',\n",
    "    '1MOK0000-74-GB' : '1MOK-00074',\n",
    "    '1MOK0000-75-NT' : '1MOK-00075',\n",
    "    '1MOK0000-79-DG' : '1MOK-00079',\n",
    "    '1MOK0000-80-DW' : '1MOK-00080',\n",
    "    '1MOK0000-81-JA' : '1MOK-00081',\n",
    "    '1MOK0000-82-AT' : '1MOK-00082',\n",
    "    '1MOK0000-82-AT' : '1MOK-00082',\n",
    "    '1MOK0000-83-KT' : '1MOK-00083',\n",
    "    '1MOK0000-83-KT' : '1MOK-00083',\n",
    "    '1MOK0000-84-AV' : '1MOK-00084',\n",
    "    '1MOK0000-84-AV' : '1MOK-00084',\n",
    "    '1MOK0088-VV' : '1MOK-00088',\n",
    "    '1MOK0088-VV' : '1MOK-00088',\n",
    "    '1Y1G-00136NF' : '1YAG-00136',\n",
    "    'YOL-00002--DD' : '1YOL-00002',\n",
    "    '2ADI-OOO60-SK' : '2ADI-00060',\n",
    "    '2JA0078' : '2DJA-00078',\n",
    "    'DJA-00233' : '2DJA-00233',\n",
    "    'DJA-00234' : '2DJA-00234',\n",
    "    'DJA-00258' : '2DJA-00258',\n",
    "    'ESP-00004' : '2ESP-00004',\n",
    "    'ESP-00006' : '2ESP-00006',\n",
    "    '4BRH-OO510-BV' : '4BRH-00510',\n",
    "    '4BRH--1260-UB' : '4BRH-01260',\n",
    "    '4BR4H-01267-SN' : '4BRH-01267',\n",
    "    '4BR4H-01267-SN' : '4BRH-01267',\n",
    "    '4NHC=-00035-ES' : '4NHC-00035',\n",
    "    '4WWUM-00033-IB' : '4WUM-00033',\n",
    "    'LALL-00029-MP' : '6ALL-00029',\n",
    "    'MBI-00254-FK' : '6MBI-00254',\n",
    "    '6MB-00516-NS-SPU' : '6MBP-00516',\n",
    "    'MBP-01654-NK' : '6MBP-01654',\n",
    "    'MBP-01654-NK' : '6MBP-01654',\n",
    "    'MBP-02816-PH' : '6MBP-02816',\n",
    "    'MBP-03862-DA' : '6MBP-03862',\n",
    "    'BRH-00133-VM' : '4BRH-00133',\n",
    "    \"BRH-00119-TR\" : \"4BRH-00119\",\n",
    "    \"1D1R-00019-KN\" : \"1DAR-00019\",\n",
    "    \"1D1R-00024-NM\" : \"1DAR-00024\",\n",
    "    \"1D1R-00020-AA\" : \"1DAR-00020\",\n",
    "    \"DOG-00020-AT\" : \"1DOG-00020\",\n",
    "    \"DOG-00019-HB\" : \"1DOG-00019\",\n",
    "    \"1DJK-00016-SPU-1\" : \"1DJK-00016\",\n",
    "    \"1DJK-00014-SPU-1\" : \"1DJK-00014\",\n",
    "    \"1DJK-00003-SPU-1\"  : \"1DJK-00003\",\n",
    "    \"1DJK-00001-SPU-1\" : \"1DJK-00001\",\n",
    "    \"1DJK-00005-SPU-1\" : \"1DJK-00005\",\n",
    "    '1MOK0000-42-MM' :'1MOK-00042',\n",
    "    '1MOK0000-41-FK' :'1MOK-00041',\n",
    "    '1MKL0000-40-EM' :'1MOK-00040',\n",
    "    '1MOK0000-60-MM' :'1MOK-00060',\n",
    "    '1MOK0000-27-ZD' :'1MOK-00027',\n",
    "    '1MOK0000-59-KL' :'1MOK-00059',\n",
    "    '1MOK00042-MM' :'1MOK-00042',\n",
    "    '1MKL000041-FK' :'1MKL-000041',\n",
    "    '1MKL000040-EM' :'1MKL-00040',\n",
    "    '1DJK 00016' :'1DJK-00016',\n",
    "    '1DJK 000-14' :'1DJK-00014',\n",
    "    '1DJK 00003' :'1DJK-00003',\n",
    "    '1DJK 00001' :'1DJK-00001',\n",
    "    '1DJK 00005' :'1DJK-00005',\n",
    "    \"1MDM-00001MDM-00018-KO\" : \"1MDM-00018\",\n",
    "    \"ESP-00008\" : \"2ESP-00008\",\n",
    "    \"ESP-00009\" : \"2ESP-00009\",\n",
    "    \"ESP-OOOO10\" : \"2ESP-00010\",\n",
    "    \"6MBP02645-NN\" : \"6MBP-02645\",\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define other Minor Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of Notes to replace with noting\n",
    "notes_to_replace_with_nothing = [\n",
    "    'P-047-SN',\n",
    "    'P-038-NN',\n",
    "    'P-059-LR',\n",
    "    'P012-3-0-0',\n",
    "    'P-00-OT',\n",
    "    'P-030-ZA',\n",
    "    'P-00-OT',\n",
    "    '1MOK000-12-JF-13-ZB-14-ME',\n",
    "    '1MOK000-12-JF-13-ZB-14-ME',\n",
    "    '1MOK000-12-JF-13-ZB-14-ME',\n",
    "    '1DJK-00491-',\n",
    "    'LS 61493-PS-1SPU-1',\n",
    "    \"POOL44-2712GORSOU ETIENNE2718ZARA KOURAMA2719MAIGONWA MARTINE2720MAHAMAD\", \n",
    "    \"LS 61493-PS-1SPU-1\",\n",
    "    \"POOL44-2712GORSOU ETIENNE2718ZARA KOURAMA2719MAIGONWA MARTINE2720MAHAMAD\",\n",
    "    \"HAMADOU SOULEY A et B\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Combined Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the two data frames together\n",
    "df = pd.concat([dfe,dff])\n",
    "\n",
    "# create a dictionary to rename the columns using partial replacement\n",
    "detail_dic = {\"RÉUSSITE\" : 'PASS',\n",
    "\t\"ÉCHEC\" : 'FAIL',\n",
    "\t'SO,' : 'NA,',\n",
    "\t\"SO\" : 'NA'\n",
    "}\n",
    "# do partial replacement of the detail columns\n",
    "special_columns = ['SPC', 'IS1081-IS6110', 'rpoB1', 'rpoB2', 'rpoB3', 'rpoB4']\n",
    "df[special_columns] = df[special_columns].replace(detail_dic, regex=True)\n",
    "\n",
    "#extract only TB samples\n",
    "df = df[df['Assay']=='Xpert MTB-RIF Ultra']\n",
    "\n",
    "# determine pool IDs that are wrongly placed in the Patient ID field\n",
    "\n",
    "def replace_sample_id_if_match(row, id_list):\n",
    "    # Convert row['Patient ID'] to string and strip any whitespace\n",
    "    patient_id = str(row['Patient ID']).strip()\n",
    "\n",
    "    # Check if the formatted patient_id is in the list\n",
    "    if patient_id in id_list:\n",
    "        return patient_id\n",
    "    else:\n",
    "        return row['Sample ID']\n",
    "\n",
    "# Update the Sample ID with the Patient ID value if it matches a pool ID\n",
    "df['Sample ID'] = df.apply(lambda row: replace_sample_id_if_match(row, patient_id_list), axis=1)\n",
    "\n",
    "# replace values in the SAMPLE ID using the replacement dictionary\n",
    "\n",
    "df['Sample ID'] = df['Sample ID'].replace(pool_id_dict)\n",
    "\n",
    "# define a function to extract the pool ID from the sample ID\n",
    "def extract_pool_id(sample_id):\n",
    "    \"\"\"\n",
    "    This function extracts the potential pool ID from the sample ID.\n",
    "    - It returns None for sample IDs that start with a number from 1 to 6 followed by 3 letters.\n",
    "    - It extracts and returns pool IDs that start with 'P' (case-insensitive) or 'S' followed by 3 digits and a dash.\n",
    "    \"\"\"\n",
    "    if pd.isnull(sample_id) or not isinstance(sample_id, str):\n",
    "        return None\n",
    "    \n",
    "    # Pattern to exclude sample IDs that start with a number from 1 to 6, followed by 3 letters\n",
    "    individual_pattern = r'^[1-6][A-Za-z]{3}'\n",
    "    \n",
    "    # If the sample_id matches the individual_pattern, return None immediately\n",
    "    if re.match(individual_pattern, sample_id):\n",
    "        return None\n",
    "\n",
    "    # Regular expression to capture pool patterns starting with \"P\" (case insensitive) or \"S\" followed by 3 digits and a dash\n",
    "    pool_pattern = r'^(?:[Pp].*|[Ss]\\d{3,}-.*)'\n",
    "    matches = re.match(pool_pattern, sample_id)\n",
    "    if matches:\n",
    "        return matches.group(0)\n",
    "    return None\n",
    "\n",
    "# Extract potential pool IDs and save in a new column 'extracted_pool_ids'\n",
    "df['extracted_pool_ids'] = df['Sample ID'].apply(extract_pool_id)\n",
    "\n",
    "# function to clean the extracted pool IDs\n",
    "def replace_space_in_extracted_id(extracted_id):\n",
    "    \"\"\"\n",
    "    This function cleans up the extracted_pool_ids based on specific patterns.\n",
    "    \"\"\"\n",
    "    if pd.isnull(extracted_id):\n",
    "        return extracted_id\n",
    "\n",
    "    # Pattern that matches a string starting with P or PS followed by at least two digits and then a space\n",
    "    pattern1 = r'^(P|PS)\\d{2,}\\s'\n",
    "    # Pattern that matches a string starting with P or p or PS followed by a space and then at least two digits\n",
    "    pattern2 = r'^(P|p|PS)\\s\\d{2,}'\n",
    "    # Pattern that matches a string starting with P or PS followed by a dash and then at least two digits\n",
    "    pattern3 = r'^(P|PS)-\\d{2,}'\n",
    "\n",
    "    if re.match(pattern1, extracted_id):\n",
    "        extracted_id = extracted_id.replace(\" \", \"-\", 1)\n",
    "    elif re.match(pattern2, extracted_id):\n",
    "        extracted_id = extracted_id.replace(\" \", \"\", 1)\n",
    "    elif re.match(pattern3, extracted_id):\n",
    "        extracted_id = extracted_id.replace(\"-\", \"\", 1)\n",
    "    \n",
    "    # Remove all occurrences of -R and -r\n",
    "    extracted_id1 = extracted_id.replace(\"-R\", \"\").replace(\"-r\", \"\").replace(\"(R)\",\"\").replace(\"RR\",\"\")\n",
    "    \n",
    "    # Replace consecutive dashes with a single dash\n",
    "    extracted_id2 = re.sub(r'-+', '-', extracted_id1)\n",
    "    # clean the extracted id\n",
    "    extracted_id3 = extracted_id2.strip()\n",
    "\n",
    "    # replace = with dash in extracted_id3\n",
    "    extracted_id4 = extracted_id3.replace(\"=\",\"-\")\n",
    "\n",
    "    # replace ; with nothing in extracted_id4\n",
    "    extracted_id5 = extracted_id4.replace(\";\", \"\")\n",
    "    # remove PATIENT DE REDCAP\n",
    "    extracted_id6 = extracted_id5.replace(\"PATIENT DE REDCAP\", \"\")\n",
    "\n",
    "    # Replace the subsequent space with nothing\n",
    "    extracted_id7 = extracted_id6.replace(\" \",\"\")\n",
    "\n",
    "    return extracted_id7\n",
    "\n",
    "\n",
    "# Apply the function to the 'extracted_pool_ids' column\n",
    "df['extracted_pool_ids'] = df['extracted_pool_ids'].apply(replace_space_in_extracted_id)\n",
    "\n",
    "\n",
    "# define a function to remove the left over spaces and other foriegn characters\n",
    "def replace_spaces_and_double_dashes(text):\n",
    "    \"\"\"\n",
    "    Replace all spaces within a string with a dash and then replace all double dashes with a single dash.\n",
    "    \n",
    "    Parameters:\n",
    "    - s (str): Input string\n",
    "    \n",
    "    Returns:\n",
    "    - str: Modified string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text1 = text.replace(\" \", \"-\")  # Replace spaces with dashes\n",
    "    text2 = text1.replace(\"--\", \"-\")  # Replace double dashes with a single dash\n",
    "    text3 = text2.replace(\"-R\",\"\") # replaece R character\n",
    "    text4 = text3.replace(\"R\",\"\")\n",
    "    text5 = text4.replace(\"=\", \"-\") # Replace equal to sign with dash\n",
    "    text6 = text5.replace(\"-2DJA-\", \"-\")\n",
    "    text7 = text6.strip() # Remove spaces \n",
    "    text8 = text7.replace(\"-()\",\"\")\n",
    "    return text8\n",
    "\n",
    "# Test\n",
    "df['extracted_pool_ids'] = df['extracted_pool_ids'].apply(replace_spaces_and_double_dashes)\n",
    "\n",
    "\n",
    "# DEFINE A FUNCTION FOR CLEANING THE NOTES FIELD\n",
    "def clean_notes(notes):\n",
    "    if pd.isnull(notes) or not isinstance(notes, str):\n",
    "        return notes\n",
    "    \n",
    "    # Remove everything from \"user\" till the end\n",
    "    notes1 = re.sub(r'user.*$', '', notes, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Split on comma and retain the first part\n",
    "    notes2 = notes1.replace(\",\",\" \")\n",
    "    \n",
    "    # Trim leading and trailing spaces\n",
    "    notes3 = notes2.strip()\n",
    "    \n",
    "    # Insert a dash after the three letters for the specified pattern\n",
    "    notes4 = re.sub(r'(?<=[1-6][A-Za-z]{3})(?=\\d{5})', '-', notes3)\n",
    "    \n",
    "    # Replace space with a dash for the specified pattern\n",
    "    notes5 = re.sub(r'(?<=[1-6][A-Za-z]{3})\\s', '-', notes4)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    notes6 = re.sub(r'\\s+', ' ', notes5)\n",
    "    notes7 = notes6.replace(\"LS:\" , '')\n",
    "    notes8 = notes7.replace(\"LS \" , ' ')    \n",
    "    notes9 = notes8.replace(\" LS \" , ' ')\n",
    "    notes10 = notes9.replace(\"ls \" , ' ')   \n",
    "    notes11 = notes10.replace(\" ls \" , ' ')\n",
    "    notes12 = notes11.replace(\"ls:\" , ' ')\n",
    "    notes13 = notes12.replace(\",\" , ' ')\n",
    "    notes14 = notes13.replace(\"  \" , ' ')\n",
    "    notes15 = notes14.strip()\n",
    "    notes16 = notes15.replace(\".\" , '-')\n",
    "    \n",
    "    return notes16\n",
    "\n",
    "def remove_names_and_dates(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Find the last occurrence of a dash followed by any characters and then a space\n",
    "    pattern = r'-(?:[^-]*?)(\\s)([^-]*$)'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        text = text[:match.start(1)].strip()  # Remove everything from the found space to the end\n",
    "\n",
    "    return text\n",
    "\n",
    "# Applying the function to the 'Notes' column\n",
    "df['clean_notes'] = df['Notes'].apply(clean_notes)\n",
    "\n",
    "# Re-applying the function to the 'clean_notes' column\n",
    "df['clean_notes'] = df['clean_notes'].apply(remove_names_and_dates)\n",
    "\n",
    "# Replace specified notes with NaN\n",
    "df['clean_notes'] = df['clean_notes'].replace(notes_to_replace_with_nothing, np.nan)\n",
    "\n",
    "# Replace 'extracted_pool_ids' with NaN where 'Notes' field is blank and 'extracted_pool_ids' is not null\n",
    "df.loc[(df['clean_notes'] == '') & (df['extracted_pool_ids'].notnull()), 'extracted_pool_ids'] = np.nan\n",
    "\n",
    "# replace the cleaned notes with the dictionary\n",
    "df['clean_notes'] = df['clean_notes'].replace(clean_notes_replacement_dict)\n",
    "\n",
    "# drop the rows where extracted_pool_ids are not blank but the clean notes is blank\n",
    "# Condition to drop rows\n",
    "condition = (df['extracted_pool_ids'].notna()) & (df['clean_notes'].isna())\n",
    "\n",
    "# Dropping rows that meet the condition\n",
    "df = df.drop(df[condition].index)\n",
    "\n",
    "#calculate the pool size by counting the number of spaces in the clean notes field\n",
    "def count_spaces_in_notes(notes, extracted_pool_ids):\n",
    "    \"\"\"\n",
    "    Count the number of spaces in the notes field after cleaning and trimming,\n",
    "    but only if extracted_pool_ids is not null.\n",
    "\n",
    "    :param notes: The notes field (string).\n",
    "    :param extracted_pool_ids: The extracted pool IDs field.\n",
    "    :return: The number of spaces in the cleaned and trimmed notes field if extracted_pool_ids is not null, otherwise 0.\n",
    "    \"\"\"\n",
    "    if pd.isnull(notes) or pd.isnull(extracted_pool_ids):\n",
    "        return 0\n",
    "\n",
    "    # Clean and trim the notes field\n",
    "    cleaned_notes = notes.strip()\n",
    "\n",
    "    # Replace double spaces with a single space\n",
    "    while \"  \" in cleaned_notes:\n",
    "        cleaned_notes = cleaned_notes.replace(\"  \", \" \")\n",
    "\n",
    "    # Count the number of spaces\n",
    "    return cleaned_notes.count(\" \")\n",
    "\n",
    "\n",
    "# Apply the function to create a new column with the number of spaces\n",
    "df['pool_size'] = df.apply(lambda x: count_spaces_in_notes(x['clean_notes'], x['extracted_pool_ids']), axis=1) +1\n",
    "\n",
    "#update the pool size to 1 if the clean notes field is empty\n",
    "def set_pool_size(row):\n",
    "    if pd.isnull(row['clean_notes']) or pd.isnull(row['extracted_pool_ids']):\n",
    "        return 1\n",
    "    else:\n",
    "        return row['pool_size']\n",
    "\n",
    "# Apply the function to each row\n",
    "df['pool_size'] = df.apply(set_pool_size, axis=1)\n",
    "\n",
    "# create a column is_pool if pool size is greater than 1\n",
    "df['is_pool'] = df['pool_size'] > 1\n",
    "\n",
    "\n",
    "# classify results into groups\n",
    "error_results = ['ERROR', 'NO RESULT', 'INVALID']\n",
    "\n",
    "#positive result groups\n",
    "positive_results = [\n",
    "        'MTB DETECTED MEDIUM||RIF Resistance NOT DETECTED',\n",
    "        'MTB DETECTED HIGH||RIF Resistance NOT DETECTED', \n",
    "        'MTB DETECTED LOW||RIF Resistance NOT DETECTED',\n",
    "        'MTB DETECTED VERY LOW||RIF Resistance NOT DETECTED',\n",
    "        'MTB DETECTED MEDIUM||RIF Resistance DETECTED',\n",
    "        'MTB DETECTED LOW||RIF Resistance DETECTED',\n",
    "        'MTB DETECTED HIGH||RIF Resistance DETECTED',\n",
    "        '|MTB Trace DETECTED|RIF Resistance INDETERMINATE', \n",
    "        'MTB DETECTED VERY LOW||RIF Resistance DETECTED',\n",
    "        'MTB DETECTED LOW||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED HIGH||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED VERY LOW||RIF Resistance INDETERMINATE',\n",
    "        'MTB Trace DETECTED|RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED MEDIUM||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED VERY LOW||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED HIGH||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED MEDIUM||RIF Resistance DETECTED',\n",
    "        'MTB DETECTED LOW||RIF Resistance INDETERMINATE',\n",
    "       \n",
    "       \n",
    "       ]\n",
    "\n",
    "#negative result groups\n",
    "negative_results = ['MTB NOT DETECTED||']\n",
    "\n",
    "#function to classify the test results\n",
    "def classify_test_result(row):\n",
    "    # Check the 'is_pool' value of the current row\n",
    "    if row['is_pool'] == False:\n",
    "        return 'individual'\n",
    "\n",
    "    if row['Test Result'] in error_results:\n",
    "        return 'error'\n",
    "    elif row['Test Result'] in positive_results:\n",
    "        return 'positive'\n",
    "    elif row['Test Result'] in negative_results:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Applying the function to the DataFrame\n",
    "df['pool_result'] = df.apply(classify_test_result, axis=1)\n",
    "\n",
    "# segment the data frames by pools\n",
    "df_pool = df[df['is_pool']==True]\n",
    "df_individual = df[df['is_pool']==False]\n",
    "\n",
    "# make a copy of the clean notes field\n",
    "df_pool['clean_notes_exploded'] = df_pool['clean_notes']\n",
    "\n",
    "# define a function to explode the dataframe using the Clean Notes field\n",
    "def explode_dataframe(df_pool):\n",
    "    # Split the 'clean_notes' column by space and explode it\n",
    "    df_exploded = df_pool.assign(clean_notes=df_pool['clean_notes'].str.split(' ')).explode('clean_notes')\n",
    "    return df_exploded\n",
    "df_pool = explode_dataframe(df_pool)\n",
    "\n",
    "df = pd.concat([df_pool, df_individual])\n",
    "\n",
    "\n",
    "# define a function to further clean the Notes field to enable the extraction of the QR Code\n",
    "def further_clean_notes(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Handle the format without a dash\n",
    "    pattern1 = r'([1-6][A-Za-z]{3})(\\d{1,4})(?!\\d|-)'\n",
    "    replacement1 = lambda m: m.group(1) + '-' + m.group(2).zfill(5)\n",
    "    text = re.sub(pattern1, replacement1, text)\n",
    "\n",
    "    # Handle the format with a dash but less than 5 digits after the dash\n",
    "    pattern2 = r'([1-6][A-Za-z]{3}-)(\\d{1,4})(?!\\d)'\n",
    "    replacement2 = lambda m: m.group(1) + m.group(2).zfill(5)\n",
    "    text2 = re.sub(pattern2, replacement2, text)\n",
    "\n",
    "    # Handle the format with 6 digits after the dash, where the first digit is 0\n",
    "    pattern3 = r'([1-6][A-Za-z]{3}-)0(\\d{5})'\n",
    "    replacement3 = lambda m: m.group(1) + m.group(2)\n",
    "    text3 = re.sub(pattern3, replacement3, text2)\n",
    "\n",
    "    return text3\n",
    "\n",
    "#clean the clean notes fied to extract qr codes\n",
    "df['clean_notes'] = df['clean_notes'].apply(further_clean_notes)\n",
    "df['clean_notes'] = df['clean_notes'].apply(further_clean_notes)\n",
    "df['Sample ID'] = df['Sample ID'].apply(further_clean_notes)\n",
    "df['Patient ID'] = df['Patient ID'].apply(further_clean_notes)\n",
    "\n",
    "df['clean_notes'] = df['clean_notes'].str.upper()\n",
    "df['Sample ID'] = df['Sample ID'].str.upper()\n",
    "df['Patient ID'] = df['Patient ID'].str.upper()\n",
    "\n",
    "# Replace the wrongly typed qr code with the qr_code_replacement_dict\n",
    "df['clean_notes'] = df['clean_notes'].replace(qr_code_replacement_dict)\n",
    "\n",
    "# do the replacement in Sample ID field\n",
    "df['Sample ID'] = df['Sample ID'].replace(qr_code_replacement_dict)\n",
    "\n",
    "# do the replacement in Patient ID field\n",
    "df['Patient ID'] = df['Patient ID'].replace(qr_code_replacement_dict)\n",
    "\n",
    "# define a function to extract the QR Code\n",
    "def extract_qr_codes(df):\n",
    "    # Define the regex pattern for a QR code\n",
    "    qr_pattern = r'([1-6][A-Z]{3}-\\d{5})'\n",
    "    \n",
    "    def get_qr_code(row):\n",
    "        # First attempt to extract from the 'clean_notes' column if 'is_pool' is True\n",
    "        if row['is_pool']:\n",
    "            match = re.search(qr_pattern, str(row['clean_notes']))\n",
    "            if match:\n",
    "                return match.group(0)\n",
    "        \n",
    "        # If not found in 'clean_notes' or 'is_pool' is False, try the 'Sample ID' column\n",
    "        match = re.search(qr_pattern, str(row['Sample ID']))\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        \n",
    "        # If not found in 'Sample ID', try the 'Patient ID' column\n",
    "        match = re.search(qr_pattern, str(row['Patient ID']))\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        \n",
    "        return None  # If no QR code found in any of the fields\n",
    "\n",
    "    # Extract QR codes based on the function\n",
    "    df['qr_codes'] = df.apply(get_qr_code, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = extract_qr_codes(df)\n",
    "\n",
    "\n",
    "# Function to extract a QR code from a single row\n",
    "def get_qr_code2(row):\n",
    "    qr_pattern = r'([1-6][A-Z]{3}-\\d{5})'\n",
    "\n",
    "    # Check 'Sample ID' for a QR code\n",
    "    match = re.search(qr_pattern, str(row['Sample ID']))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    \n",
    "    # If not found, check 'Patient ID'\n",
    "    match = re.search(qr_pattern, str(row['Patient ID']))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "\n",
    "    # If still not found, check 'clean_notes'\n",
    "    match = re.search(qr_pattern, str(row['clean_notes']))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    \n",
    "    return None  # Return None if no QR code found\n",
    "\n",
    "# Update 'qr_codes' where it is currently null\n",
    "mask = df['qr_codes'].isnull()\n",
    "df.loc[mask, 'qr_codes'] = df[mask].apply(get_qr_code2, axis=1)\n",
    "\n",
    "# Now df contains updated 'qr_codes' values\n",
    "# define a function to extract the S4A QR Code\n",
    "def extract_s4a_qr_codes(row, columns_to_search):\n",
    "    \"\"\"\n",
    "    Extract the first S4A QR code from specified columns of a DataFrame row.\n",
    "\n",
    "    :param row: A row of the DataFrame.\n",
    "    :param columns_to_search: List of column names to search for the QR code.\n",
    "    :return: The first extracted S4A QR code or NaN if none found.\n",
    "    \"\"\"\n",
    "    pattern = r'SC[1-5]-\\d{4}'\n",
    "\n",
    "    for column in columns_to_search:\n",
    "        if not isinstance(row[column], str):\n",
    "            continue\n",
    "\n",
    "        match = re.search(pattern, row[column])\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Define columns to search for QR codes\n",
    "columns_to_search = ['clean_notes', 'Patient ID', 'Sample ID']\n",
    "\n",
    "# Apply the function to create a new column with the extracted S4A QR codes\n",
    "df['s4a'] = df.apply(extract_s4a_qr_codes, args=(columns_to_search,), axis=1)\n",
    "\n",
    "# Define a function to determine the project based on s4a and qr_codes\n",
    "def determine_project(row):\n",
    "    if pd.notnull(row['s4a']):\n",
    "        return 'S4A'\n",
    "    elif pd.notnull(row['qr_codes']):\n",
    "        return 'Inspire'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the function to create a new column 'project'\n",
    "df['project'] = df.apply(determine_project, axis=1)\n",
    "\n",
    "# Update 'qr_codes' to the value of 's4a' if 'qr_codes' is null\n",
    "df['qr_codes'] = df.apply(lambda row: row['s4a'] if pd.isnull(row['qr_codes']) else row['qr_codes'], axis=1)\n",
    "\n",
    "# define a function to update the qr code in place\n",
    "def update_qr_codes_inplace(df):\n",
    "    \"\"\"\n",
    "    Update the qr_codes column in the DataFrame in place.\n",
    "    If is_pool is True and qr_codes is null, set qr_codes to clean_notes.\n",
    "    Otherwise, set qr_codes to Sample ID where qr_codes is null.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to be updated.\n",
    "    \"\"\"\n",
    "    # Compute the values to be assigned\n",
    "    values_to_assign = np.where(\n",
    "        df['is_pool'] & df['qr_codes'].isnull(), \n",
    "        df['clean_notes'], \n",
    "        df['Sample ID']\n",
    "    )\n",
    "\n",
    "    # Filter the values to match the length of rows where 'qr_codes' is null\n",
    "    values_to_assign = values_to_assign[df['qr_codes'].isnull()]\n",
    "\n",
    "    # Assign the computed values to 'qr_codes' where 'qr_codes' is null\n",
    "    df.loc[df['qr_codes'].isnull(), 'qr_codes'] = values_to_assign\n",
    "\n",
    "update_qr_codes_inplace(df)\n",
    "\n",
    "# Merge the xpert files with the main dataframe\n",
    "df = df.merge(xpert_files[['file', 'lab', 'start_date', 'end_date', 'date_created']], on = 'file', how = 'left')\n",
    "\n",
    "# Convert 'date_created' to datetime format\n",
    "df['date_created'] = pd.to_datetime(df['date_created'])\n",
    "\n",
    "# Sort by 'Cartridge S/N' and 'date_created' in descending order\n",
    "df = df.sort_values(by=['Cartridge S/N', 'date_created'], ascending=[True, False])\n",
    "\n",
    "# Group by 'Cartridge S/N' and keep only the rows with the latest 'date_created' for each group\n",
    "df = df[df['date_created'] == df.groupby('Cartridge S/N')['date_created'].transform('max')]\n",
    "\n",
    "# sort the dataframe by the pool result, pool size, and qr codes\n",
    "df.sort_values(by =['is_pool','Cartridge S/N', 'file'], ascending=[False, True, True], inplace = True)\n",
    "df = df.drop_duplicates(subset = ['Cartridge S/N','qr_codes'])\n",
    "df.dropna(subset = ['qr_codes'], inplace = True)\n",
    "\n",
    "# Define the desired categories order\n",
    "categories_order = ['positive', 'negative', 'individual', 'error']\n",
    "\n",
    "# Convert the 'pool_result' column to categorical with the specified order\n",
    "df['pool_result'] = pd.Categorical(df['pool_result'], categories=categories_order, ordered=True)\n",
    "\n",
    "# Convert the 'pool_result' column to categorical with the specified order\n",
    "df['pool_result'] = pd.Categorical(df['pool_result'], categories=categories_order, ordered=True)\n",
    "\n",
    "#drop duplicates\n",
    "df.drop_duplicates(subset = ['Cartridge S/N', 'qr_codes'], inplace = True)\n",
    "\n",
    "df.sort_values(by=['date_created','is_pool', 'pool_result', 'qr_codes',], ascending=[False, False, True, True,], inplace=True)\n",
    "\n",
    "#determine the duplicate number for each qr code\n",
    "df['dup_no'] = df.groupby('qr_codes').cumcount() + 1\n",
    "# Sort the dataframe\n",
    "df.sort_values(by=['pool_result', 'qr_codes', 'dup_no'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "#Write a function to get the individual results\n",
    "def get_individual_values(row, df):\n",
    "    # Check if the row is a positive pooled sample\n",
    "    if row['is_pool'] and row['pool_result'] == 'positive':\n",
    "        # Create a subset DataFrame for individual tests matching the QR code of the pool\n",
    "        subset_df = df[(df['qr_codes'] == row['qr_codes']) & (~df['is_pool'])]\n",
    "        # If individual tests are found, return the Test Result of the first one\n",
    "        if not subset_df.empty:\n",
    "            return subset_df['Test Result'].iloc[0]\n",
    "        # If no individual tests are found, return \"No Individual Results Yet\"\n",
    "        else:\n",
    "            return \"No Ind Test Yet\"\n",
    "    # For all other cases, return the Test Result of the row\n",
    "    return row['Test Result']\n",
    "\n",
    "\n",
    "# Apply the function to the 'Test Result' column of df_valid\n",
    "df['individual_results'] = df.apply(lambda row: get_individual_values(row, df), axis=1)\n",
    "\n",
    "# function to get the individual cartridge SN\n",
    "def get_individual_cartridge_sn(row):\n",
    "    # If the row represents a pool and the result is positive\n",
    "    if row['is_pool'] and row['pool_result'] == 'positive':\n",
    "        # Filter df to find the corresponding row where 'is_pool' is False\n",
    "        subset_df = df[(df['qr_codes'] == row['qr_codes']) & (~df['is_pool'])]\n",
    "\n",
    "        # If there's a match, return the 'Cartridge S/N' value from the subset\n",
    "        if not subset_df.empty:\n",
    "            return subset_df['Cartridge S/N'].iloc[0]\n",
    "        # If no individual test is found, return \"No Individual Results Yet\"\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # For all other cases (including pools that are not positive), return None\n",
    "    return None\n",
    "\n",
    "df['individual_cartridge_sn'] = df.apply(lambda row: get_individual_cartridge_sn(row), axis=1)\n",
    "\n",
    "# Define the categories based on individual results\n",
    "def categorize_result(individual_result):\n",
    "    if individual_result in positive_results:\n",
    "        return 'positive'\n",
    "    elif individual_result in negative_results:\n",
    "        return 'negative'\n",
    "    elif individual_result in error_results:\n",
    "        return 'invalid'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "df['indiv_result_category'] = df['individual_results'].apply(categorize_result)\n",
    "\n",
    "\n",
    "#create the final pool size for individuals where the pool was positive\n",
    "def update_pool_size(df):\n",
    "    # Create a new column 'final_pool_size' with the same values as 'pool_size' initially\n",
    "    df['final_pool_size'] = df['pool_size']\n",
    "    \n",
    "    # Find the unique xpert_ids with more than one occurrence\n",
    "    xpert_ids = df['qr_codes'].value_counts()\n",
    "    xpert_ids = xpert_ids[xpert_ids > 1].index.tolist()\n",
    "    \n",
    "    # Iterate through each unique xpert_id\n",
    "    for xpert_id in xpert_ids:\n",
    "        # Check if xpert_id has both is_pool True and False entries\n",
    "        if df[(df['qr_codes'] == xpert_id) & (df['is_pool'])].empty or df[(df['qr_codes'] == xpert_id) & (~df['is_pool'])].empty:\n",
    "            continue  # Skip ids that don't have both True and False entries\n",
    "        \n",
    "        # Get the pool size where is_pool is True\n",
    "        true_pool_size = df[(df['qr_codes'] == xpert_id) & (df['is_pool'])]['pool_size'].iloc[0]\n",
    "        \n",
    "        # Update the 'final_pool_size' where is_pool is False for the same xpert_id\n",
    "        df.loc[(df['qr_codes'] == xpert_id) & (~df['is_pool']), 'final_pool_size'] = true_pool_size\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Now call the function to update the DataFrame\n",
    "df = update_pool_size(df) \n",
    "\n",
    "# if final pool size  = 1, then test type = individual else pooled\n",
    "df['test_type'] = np.where(df['final_pool_size']==1, 'individual', 'pooled')\n",
    "\n",
    "# Define the QR code pattern\n",
    "qr_pattern = r'([1-6][A-Z]{3}-\\d{5})'\n",
    "\n",
    "# Function to extract the first four characters if qr_code matches the pattern\n",
    "def extract_site(qr_code):\n",
    "    if re.match(qr_pattern, qr_code):\n",
    "        return qr_code[:4]\n",
    "    else:\n",
    "        return None  # Or return '' if you prefer an empty string for non-matches\n",
    "\n",
    "# Create the 'site' column based on 'qr_codes'\n",
    "df['site'] = df['qr_codes'].apply(extract_site)\n",
    "\n",
    "# determine if someone has TB\n",
    "df['has_tb'] = df['indiv_result_category'] == 'positive'\n",
    "\n",
    "# where site is not null, xpert_id = qr_codes\n",
    "df['xpert_id'] = np.where(df['site'].notnull(), df['qr_codes'], np.nan)\n",
    "\n",
    "# if both extracted_pool_ids and Notes are notnull, then pool_id = extracted_pool_ids\n",
    "df['pool_id'] = np.where(df['extracted_pool_ids'].notnull() & df['Notes'].notnull(), df['extracted_pool_ids'], np.nan)\n",
    "\n",
    "\n",
    "# convert datetime columns to datetime\n",
    "df['Expiration Date'] = df['Expiration Date'].apply(fix_dates)\n",
    "df['Start Time'] = df['Start Time'].apply(fix_dates)\n",
    "df['End Time'] = df['End Time'].apply(fix_dates)\n",
    "# Define the date for comparison\n",
    "date_threshold = pd.Timestamp('2023-06-27')\n",
    "current_date = pd.Timestamp.today()\n",
    "\n",
    "# Create the 'date' column based on the conditions\n",
    "df['date'] = np.where(\n",
    "    df['End Time'] < date_threshold, df['start_date'],\n",
    "    np.where(df['End Time'] > current_date, df['end_date'], df['End Time']))\n",
    "\n",
    "#fill missing values in the date column with the enddate value\n",
    "df['date'] = df['date'].fillna(df['end_date'])\n",
    "\n",
    "def classify_pool(group):\n",
    "    \"\"\"\n",
    "    Classify the pool based on individual test results within the group.\n",
    "    \"\"\"\n",
    "    if group['pool_result'].iloc[0] == 'error':\n",
    "        return 'Error'\n",
    "    elif group['pool_result'].iloc[0] == 'negative':\n",
    "        return 'Negative'\n",
    "    elif group['pool_result'].iloc[0] == 'positive':\n",
    "        if all(group['indiv_result_category'] == 'negative'):\n",
    "            return 'False Positive'\n",
    "        elif any(group['indiv_result_category'] == 'positive'):\n",
    "            return 'True Positive'\n",
    "        elif all((group['indiv_result_category'] == 'negative') | (group['indiv_result_category'] == 'unknown')):\n",
    "            return 'Incomplete'\n",
    "    return 'Unknown'\n",
    "\n",
    "# Filter the DataFrame for pooled samples and apply the classification function to each pooled group\n",
    "pooled_df = df[df['is_pool']]\n",
    "pooled_classifications = pooled_df.groupby('Cartridge S/N').apply(classify_pool)\n",
    "\n",
    "# Map the classifications back to the original DataFrame for pooled samples\n",
    "df.loc[df['is_pool'], 'pool_classification'] = df['Cartridge S/N'].map(pooled_classifications)\n",
    "\n",
    "# Set pool_classification to NaN where final_pool_size is 1\n",
    "df.loc[df['final_pool_size'] == 1, 'pool_classification'] = np.nan\n",
    "\n",
    "#drop unnecessary columns\n",
    "df.drop(columns= ['rpoB1 melt', 'rpoB2 melt', 'rpoB3 melt', 'rpoB4 melt', 'rpoB1 Mut melt', 'rpoB2 Mut melt', 'rpoB3 Mut melt',\n",
    "       'rpoB4 Mut melt A', 'rpoB4 Mut melt B', 'Patient ID 2', 'Assay', 'Assay Version', 'Assay Type', 'Test Type',\n",
    "       'clean_notes', 'Module Name', 'start_date' , 's4a', 'start_date', 'end_date', 'date_created', 'Sample Type', 'clean_notes',], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
